Виртуализация сети не особо укладывается в тематику АДСМ, где мы разбираем автоматику. Но без неё может не сложиться понимания, почему сеть выглядит так, как я её буду описывать в следующей статье.
И раз уж мы об этом заговорили, то стоит упомянуть предпосылки к виртуализации сети.  

Наверно, вы не раз слышали, что сеть всегда была самой инертной частью любой системы. И это правда во всех смыслах. Сеть - это базис, на который опирается всё, и производить изменения на ней довольно сложно - сервисы не терпят, когда сеть лежит. Зачастую вывод из эксплуатации одного узла может сложить большую часть приложений и повлиять на много клиентов. Отчасти поэтому сетевая команда может сопротивляться любым изменениям - потому что сейчас оно как-то работает (мы, возможно, не знаем как), а тут надо что-то новое настроить, и неизвестно как оно повлияет на сеть.

Чтобы не ждать, когда сетевики прокинут VLAN, люди придумали использовать оверлеи - наложенные сети - коих великое многообразие. GRE, IPinIP, MPLS, VxLAN, EVPN, MPLSoverUDP, MPLSoverGRE итд.

Их прелесть заключается в двух простых вещах:
    - Настраиваются только конечные узлы - транзитные трогать не приходится
    - Нагрузка сокрыта глубоко внутри заголовков - транзитным узлам не нужно ничего знать о ней, об адресации на хостах, маршрутах наложенной сети. 

В этом полувыпуске я не планирую разбирать все возможные технологии, а скорее описать фреймворк работы оверлейных сетей.

Вся серия будет описывать ДЦ, состоящий из рядов однотипных стоек, в которых установлено одинаковое серверное оборудование. 
На этом оборудовании запускаются виртуальные машины/контейнеры/серверлесс, реализующие сервисы.

<hr>

<h1>Терминология</h1>
В статье <b>сервером</b> мы будем называть программу, которая реализует серверную сторону клиент-серверной коммуникации.
Физические машины в стойках называть серверами не будем.

<b>Физическая машина</b> - x86-компьютер, установленный в стойке. Наиболее часто употрим термин <b>хост</b>. Будем называть её "<b>машина</b>" или хост.

<b>Гипервизор</b> - приложение, запущенное на физической машине, эмулирующее физические ресурсы для Виртуальной Машины.

<b>Виртуальная машина</b> - операционная система, запущенная на физической машине. Для нас в рамках данного цикла не так уж важно, на самом ли деле это виртуальная машина или просто контейнер. Будем называть это"<b>ВМ</b>"


<b>ToR - Top of the Rack</b> - коммутатор, установленный в стойке, к котором подключены все физические машины.

<b>Underlay network</b> или подлежащая сеть или андэрлей - физическая сетевая инфраструктура: коммутаторы, маршрутизаторы, кабели.
<b>Overlay network</b> или наложенная сеть или оверлей - виртуальная сеть туннелей, работающая поверх физической.
<b>L3-фабрика или IP-фабрика</b> - потрясающее изобретение человечества, позволяющее к собеседованиям не повторять STP и вообще не учить TRILL. Концепция, в которой вся сеть вплоть до уровня доступа исключительно L3, без VLAN и соответственно огромных растянутых широковещательных доменов. Откуда тут слово "фабрика" разберёмся в следующей части.



<img src="https://fs.linkmeup.ru/images/adsm/1/terminology.png" width="700">


Сеть сегодня в большинстве случае можно разбить на две части: 
<b>Underlay</b> - физическая сеть со стабильной конфигурацией.
<b>Overlay</b> - абстракция над Underlay. 
Это верно, как для случая ДЦ (VxLAN), который мы разберём в будущем, так и для ISP (MPLS L3VPN). С энтерпрайзными сетями, конечно, ситуация несколько иная. 


Чуть более упрощённая картинка:

<img src="https://fs.linkmeup.ru/images/adsm/1/fabric.png" width="700">

<hr>

<h1>Underlay</h1>
Underlay - это физическая сеть: аппаратные коммутаторы и кабели.

!!!!!
<img src="https://fs.linkmeup.ru/images/adsm/1/underlay.png" width="700">

Опирается он на стандартные протоколы и технологии. Не в последнюю очередь потому, что аппаратные устройства по сей день работают на проприетарном ПО, не допускающем ни программирование чипа, ни реализацию своих протоколов, соответственно, нужна взаимосовместимость с другими вендорами и стандартизация.
<blockquote>
А вот кто-нибудь вроде Гугла может себе позволить разработку собственных коммутаторов и отказ от общепринятых протоколов. Но линк_май_ди_ап!!! LAN_DC не гугл.
</blockquote>

Underlay сравнительно редко меняется, потому что его задача - базовая IP-связность между физическими машинами. Underlay ничего не знает о запущенных поверх него сервисах, клиентах, тенантах - ему нужно только доставить пакет от одной машины до другой.
Underlay может быть например таким: 
<ul>
    <li>IPv4+OSPF
    Или</li> 
    <li>IPv6+ISIS+BGP+L3VPN
    Или</li>
    <li>L2+TRILL
    Или</li>
    <li>L2+STP</li>
</ul>
Настраивается Underlay'ная сеть классическим образом: CLI/GUI/NETCONF.
Вручную, скриптами, проприетарными утилитами.

<hr>
<h1>Overlay</h1>
Overlay - виртуальная сеть туннелей, натянутая поверх Underlay.


<img src="https://fs.linkmeup.ru/images/adsm/1/overlay.png" width="700">


Так ВМ одного клиента (одного сервиса) могут общаться друг с другом через Overlay, даже не подозревая какой на самом деле путь проходит пакет. 
Overlay может быть например таким:
<ul>
    <li>GRE-туннель</li>
    <li>VxLAN</li>
    <li>EVPN</li>
    <li>L3VPN</li>
</ul>
Overlay обычно настраивается и поддерживается через центральный контроллер. С него конфигурация и data plane доставляются на конечные виртуальные устройства.
Да, это SDN в чистом виде.

Overlay может начинаться на коммутаторе доступа (ToR), стоящем в стойке, как это происходит, например, в случае VxLAN. Тогда коммутатор должен разделять различные сервисы, соответственно, сетевой администратор должен в известной степени сотрудничать с администраторами виртуальных машин и вносить изменения (пусть и автоматически) в конфигурацию устройств.
!!! Попробовать самому и написать поподробнее.

Другой подход - начинать и терминировать туннели на конечных хостах.
В этом случае сеть (Underlay) остаётся максимально простой и статичный.
А хост сам делает все необходимые инкапсуляции.
Для этого потребуется, безусловно, запускать специальное приложение на хостах, но оно того стоит. 
<hr>

Проще всего рассмотреть на примерах. И в качестве подопытного мы возьмём OpenContrail, ныне известный как <a href="https://tungsten.io">Tungsten Fabric</a>.
Во-первых, у меня с ним опыта побольше, во-вторых, этот подход более изящный. 

<h2>На примере Tungsten Fabric</h2>

На каждой физической машине есть <b>vRouter</b> - виртуальный маршрутизатор, который знает о подключенных к нему сетях и каким клиентам они принадлежат. Для каждого клиента он поддерживает изолированную таблицу маршрутизации. И собственно vRouter делает Overlay'ное туннелирование.
Каждая ВМ, расположенная на гипервизоре, соединяется с vRouter'ом этой машины через <b>TAP-интерфейс</b>.


<img src="https://fs.linkmeup.ru/images/adsm/1/tf-host.png" width="600">

Если за vRouter'ом находится несколько сетей, то для каждой из них создаётся виртуальный интерфейс, на который назначается IP-адрес - он будет адресом шлюза по умолчанию.
Все сети одного клиента помещаются в один <b>VRF</b> (одну таблицу).

Чтобы vRouter'ы могли общаться друг с другом, а соответственно и ВМ, находящиеся за ними, они обмениваются маршрутной информацией через SDN-контроллер.


<a href="https://fs.linkmeup.ru/images/adsm/1/sdn-controller.png"><img src="https://fs.linkmeup.ru/images/adsm/1/sdn-controller.png" width="800"></a>

Чтобы выбраться во внешний мир, существует точка выхода из матрицы - шлюз виртуальной сети VNGW - Virtual Network GateWay (<i>термин мой</i>).


<a href="https://fs.linkmeup.ru/images/adsm/1/vngw.png"><img src="https://fs.linkmeup.ru/images/adsm/1/vngw.png" width="800"></a>

<hr>

Теперь рассмотрим примеры коммуникаций.

<h3> 1. Коммуникация внутри одной физической машины</h3>
VM1 хочет отправить пакет на VM3. Предположим пока, что это ВМ одного клиента.

<h4>Data Plane</h4>
У VM1 есть маршрут по умолчанию в его интерфейс eth0. Пакет отправляется туда.
Этот интерфейс Eth0 на самом деле виртуально соединён с виртуальным маршрутизатором vRouter через TAP-интерфейс.
vRouter анализирует на какой интерфейс пришёл пакет, то есть к какому клиенту (VRF) он относится, сверяет адрес получателя с таблицей маршрутизации этого клиента.
Обнаружив, что получатель на этой же машине за другим портом, vRouter просто отправляет пакет в него без каких-либо дополнительных заголовков.


<img src="https://fs.linkmeup.ru/images/adsm/1/intra-hv-dp.png" width="700">

Пакет в этом случае не попадает в физическую сеть.

<h4>Control Plane</h4>
Гипервизор при запуске виртуальной машины сообщает ей:
<ul>
    <li>Её собственный IP-адрес.</li>
    <li>Маршрут по умолчанию - через IP-адрес vRouter'а в этой сети.</li>
</ul>

vRouter'у через специальный API гипервизор сообщает:
<ul>
    <li>Что нужно создать виртуальный интерфейс.</li>
    <li>Какому клиенту (VRF) принадлежит этот интерфейс.</li>
    <li>Статическую ARP-запись для этой VM - за каким интерфейсом находится её IP-адрес и к какому MAC-адресу он привязан.</li>
</ul>


<img src="https://fs.linkmeup.ru/images/adsm/1/intra-hv-cp.png" width="700">

Таким образом все ВМ одного клиента на данной машине vRouter видит как непосредственно подключенные сети и может сам между ними маршрутизировать.
<hr>

А вот VM1 и VM3 принадлежат разным клиентам, соответственно, находятся  в разных таблицах vRouter'а.
Смогут ли они друг с другом общаться напрямую, зависит от настроек vRouter и дизайна сети.
Например, в случае, когда хотя бы один клиент использует только приватную адресацию коммуникация уже не получится - возможно пересечение адресных пространств.
Если оба клиента используют публичные адреса, или NAT происходит на самом vRouter'е, то можно сделать и прямую маршрутизацию.
В этом случае мы идём по вышеописанному сценарию.

А можно заставить их ходить через шлюз. Тогда это почти то же самое - что выход во внешние сети, о котором ниже.!!!
<hr>

<h3> 2. Коммуникация между ВМ, расположенными на разных физических машинах.</h3>
<h4>Data Plane</h4>
Начало точно такое же: VM1 посылает пакет с адресатом VM4 по своему дефолту.
vRouter его получает и на этот раз видит, что адресат находится на другой машине и доступен через туннель Tunnel0.
Сначала он вешает метку MPLS, идентифицирующую VRF (клиента), чтобы на обратной стороне vRouter мог определить куда этот пакет поместить.


<img src="https://fs.linkmeup.ru/images/adsm/1/inter-hv-dp.png" width="700">

Поменять картинку без серого фона!!!!

У Tunnel0 источник IP1, получатель: IP2.
vRouter добавляет заголовки GRE и новый IP к исходному пакету.
В таблице маршрутизации vRouter есть маршрут по умолчанию через адрес ToR1. Туда и отправляет


<img src="https://fs.linkmeup.ru/images/adsm/1/headers.png" width="700">

ToR как участник Underlay сети знает (Например, по OSPF), как добраться до IP2 и отправляет пакет по маршруту.
При этом знать, что находится под внешним заголовком IP ему не нужно. То есть фактически под IP может быть бутерброд из IPv6 over MPLS over Ethernet over MPLS over GRE over over over.
Соответственно на принимающей стороне vRouter снимает GRE и по MPLS-метке понимает, какому клиенту этот пакет надо передать, раздевает его и отправляет в первоначальном виде получателю.


<h4>Control Plane</h4>
Происходит всё то же, что было описано выше!!!
И плюс ещё следующее:
<ul>
    <li>Для каждого клиента vRouter выделяет MPLS-метку. Это сервисная метка L3VPN, по которой клиенты будут разделяться в пределах одной физической машины.</li>
    <li>vRouter устанавливает соединение с SDN-контроллером по протоколу BGP (или похожему на него - в случае TF -это XMPP 0_o).</li>
    <li>Через эту сессию vRouter сообщает SDN-контроллеру маршруты до подключенных сетей:
        ○ Адрес сети
        ○ MPLS-метка клиента
        ○ Свой IP-адрес в качестве nexthop.</li>
    <li>SDN-контроллер получает такие маршруты от всех подключенных vRouter, и отражает их другим. То есть он выступает Route Reflector'ом.</li>
</ul>


<a href="https://fs.linkmeup.ru/images/adsm/1/inter-hv-cp.png" target="blank"><img src="https://fs.linkmeup.ru/images/adsm/1/inter-hv-cp.png" width="700"></a>


<a href="https://fs.linkmeup.ru/images/adsm/0/inter-hv-cp.png" target="blank"><img src="https://fs.linkmeup.ru/images/adsm/0/inter-hv-cp.png" width="700"></a>

Overlay может меняться хоть каждую минуту. Примерно так это и происходит в публичных облаках, когда клиенты регулярно запускают и выключают свои виртуальные машины.
Центральный контроллер берёт на себя все сложности с поддержанием конфигурации и контролем таблиц коммутации/маршрутизации на vRouter.
Если говорить грубо, то контроллер запиривается со всеми vRouter'ами по BGP или похожему на него протоколу и просто передаёт маршрутную информацию. BGP, например, уже имеет AF!!! для передачи метода инкапсуляции MPLSoGRE или MPLSoUDP.

При этом не меняется никоим образом конфигурация Underlay-сети, которую кстати, автоматизировать на порядок сложнее, а сломать неловким движением проще.
<hr>

<h3>Выход во внешний мир</h3>
Где-то симуляция должна закончиться и из виртуального мира нужно выходить в реальный. И нужен шлюз.
Практикуют два подхода:
<ol>
    <li>Ставится аппаратный маршрутизатор.</li>
    <li>Запускается какой-либо appliance, реализующий функции маршрутизатора (да-да, вслед за SDN мы и с VNF столкнулись). Назовём его виртуальный шлюз.</li>
</ol>

<blockquote>
Преимущество второго подхода в дешёвой горизонтальной масштабируемости - не хватает мощности - запустили ещё одну виртуалку со шлюзом. На любой физической машине, без необходимости искать свободные стойки, юниты, вывода питания, покупать саму железку, везти её, устанавливать, коммутировать, настраивать, а потом ещё и менять в ней сбойные компоненты.
Минусы же у виртуального шлюза в том, что единица физического маршрутизатора всё же на порядки мощнее многоядерной виртуалки, а его софт, подогнанный под его же аппаратную основу, работает значительно стабильнее (нет). Сложно отрицать и тот факт, что программно-аппаратный комплекс просто работает, требуя только настройки, в то время как запуск и обслуживание виртуального шлюза - занятие для сильных инженеров.
</blockquote>

Одной своей ногой шлюз смотрит в виртуальную сеть Overlay, как обычная Виртуальная Машина, и может взаимодействовать со всеми другими ВМ. При этом она может терминировать на себе сети всех клиентов и, соответственно, осуществлять и маршрутизацию между ними.

Другой ногой шлюз смотрит уже в магистральную сеть и знает о том, как выбраться в Интернет.


<h4>Data Plane</h4>
То есть процесс выглядит так: 
    <ol>
    <li>VM1, имея дефолт, отправляет пакет с адресатом во внешнем мире в интерфейс eth0.</li>
    <li>vRouter получает этот пакет. Вешает MPLS-метку данного клиента.</li>
    <li>Он знает так же от контроллера, что маршрут по умолчанию лежит через шлюз VNGW1 с IP-адресом IP3.</li>
    vRouter упаковывает первоначальный пакет в заголовок GRE и отправляет на ToR.</li>
    <li>Underlay доставляет пакет до шлюза VNGW1.</li>
    <li>Шлюз VNGW1 снимает туннелирующие заголовки (GRE, MPLS), видит адрес, консультируется со своей таблицей маршрутизации и понимает, что он направлен в Интернет.</li>
    <li>От VNGW до бордера может быть обычная IP-сеть, что вряд ли.
    Может быть классическая MPLS сеть (IGP+LDP/RSVP TE), может быть обратно фабрика с BGP LU или GRE-туннель от VNGW до бордера через IP-сеть.
    Как бы то ни было VNGW1 совершает необходимые инкапсуляции и отправляет первоначальный пакет в сторону бордера.</li>
</ol>



<a href="https://fs.linkmeup.ru/images/adsm/1/outside-dp.png" target="blank"><img src="https://fs.linkmeup.ru/images/adsm/1/outside-dp.png" width="700"></a>

Трафик в обратную сторону проходит те же шаги в противоположном порядке. 
<ol>
    <li>Бордер упаковывает внешний пакет в туннель, добрасывает до VNGW1</li>
    <li>Тот его раздевает, смотрит на адрес получателя и видит, что тот доступен через туннель MPLSoGRE.</li>
    <li>Соответственно, вешает метку MPLS, заголовок GRE и отправляет на ToR.
    Адрес назначения - IP-адрес vRouter'а, за которым стоит целевая ВМ.</li>
    <li>ToR знает, как добраться до этого адреса. </li>
    <li>Целевой vRouter снимает GRE, по MPLS-метке определяет клиента и шлёт голый IP-пакет в свой TAP-интерфейс, связанный с eth0 ВМ.</li>
</ol>

<a href="https://fs.linkmeup.ru/images/adsm/1/outside-dp-reverse.png" target="blank"><img src="https://fs.linkmeup.ru/images/adsm/1/outside-dp-reverse.png" width="700"></a>

<h4>Control Plane</h4>
VNGW1 устанавливает BGP-соседство с SDN-контроллером, от которого он получает всю маршрутную информацию о клиентах: за каким IP-адресом (vRouter'ом) находится какой клиент, и какой MPLS-меткой он идентифицируется.
Аналогично он сам SDN-контроллеру сообщает дефолтный маршрут, указывая себя в качестве nexthop'а.
Точно так же, как и другие ВМ между собой, между ВМ и VNGW трафик ходит инкапсулированным в MPLSoGRE (или MPLSoUDP).

То есть и тут SDN-контроллер выступает в роли Route-Reflector'а между VNGW и vRouter'ами.

На VNGW обычно происходит агрегация маршрутов или NAT-трансляция.
И в другую сторону в сессию с бордерами или Route Reflector'ами он отдаёт именно этот агрегированный маршрут. А от них получает маршрут по умолчанию или Full-View, или что-то ещё. 

<img src="https://fs.linkmeup.ru/images/adsm/1/outside-cp.png" width="700">
!!!!Дорисовать


<hr>

А что там наш Underlay? 
А в общем-то ничего. Он всю дорогу не менялся. Всё, что ему нужно делать в Control Plane'е - это обновлять ARP'ы по мере появления и исчезновения vRouter/VNGW и таскать пакеты от между ними.


Но работе самой Underlay-сети я посвятил здесь совсем мало времени. Это потому, что далее в серии я именно на нём и сосредоточусь, а Overlay мы будем затрагивать только вскользь.

<h1>Полезные ссылки</h1>
Опенконтрейл
https://tungstenfabric.github.io/website/
